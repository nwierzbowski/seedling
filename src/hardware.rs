//! Hardware module for managing NVIDIA SMI locks and GPU configuration.
//!
//! This module replaces the safety script and handles all hardware-related
//! operations including GPU configuration, nvidia-smi locks, and system monitoring.

use std::process::Command;
use anyhow::{Result, Context};

/// Manages NVIDIA SMI locks for GPU resources.
pub struct NvSmiLockManager {
    /// Tracks whether a lock has been acquired
    lock_acquired: bool,
}

impl NvSmiLockManager {
    /// Creates a new lock manager instance.
    pub fn new() -> Self {
        Self {
            lock_acquired: false,
        }
    }

    /// Sanitizes input arguments to prevent command injection.
    fn sanitize_args(args: &[&str]) -> Vec<String> {
        args.iter()
            .map(|arg| {
                // Remove potentially dangerous characters from arguments
                arg.chars()
                    .filter(|c| !c.is_control() && *c != ';' && *c != '&' && *c != '|' && *c != '`')
                    .collect::<String>()
            })
            .collect::<Vec<String>>()
    }

    /// Validates that the nvidia-smi command exists and is executable.
    fn validate_nvidia_smi() -> Result<()> {
        let nvidia_smi_exists = Command::new("which")
            .arg("nvidia-smi")
            .output()
            .map(|o| o.status.success())?;

        if !nvidia_smi_exists {
            anyhow::bail!("nvidia-smi not found - cannot manage GPU locks");
        }

        // Also check that we can execute it with basic query
        let output = Command::new("nvidia-smi")
            .args(&["--query-gpu=count", "--format=csv,noheader"])
            .output()
            .context("Failed to validate nvidia-smi")?;

        if !output.status.success() {
            anyhow::bail!("nvidia-smi validation failed: {}", String::from_utf8_lossy(&output.stderr));
        }

        Ok(())
    }

    /// Acquires an NVIDIA SMI lock for GPU access.
    pub async fn acquire_lock(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        // Validate nvidia-smi exists and is executable
        Self::validate_nvidia_smi()?;

        // Check if we're on a system with NVIDIA GPUs
        let gpu_check = Command::new("nvidia-smi")
            .args(&["--query-gpu=count", "--format=csv,noheader"])
            .output()
            .context("Failed to check for NVIDIA GPUs")?;

        if !gpu_check.status.success() {
            anyhow::bail!("No NVIDIA GPUs detected");
        }

        // Validate the command arguments are safe
        let args = Self::sanitize_args(&["-pm", "1"]);

        // Acquire the lock by running nvidia-smi with lock flags
        let status = Command::new("sudo")
            .arg("nvidia-smi")
            .args(&args)
            .status()
            .context("Failed to execute nvidia-smi for lock acquisition")?;

        if !status.success() {
            anyhow::bail!("Failed to acquire NVIDIA SMI lock");
        }

        self.lock_acquired = true;
        println!("âœ… Acquired NVIDIA SMI lock successfully");
        Ok(())
    }

    /// Releases an NVIDIA SMI lock.
    pub async fn release_lock(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        // Only attempt to release if we have a lock
        if !self.lock_acquired {
            println!("No lock to release - no GPU lock was acquired");
            return Ok(());
        }

        // Validate nvidia-smi exists and is executable
        Self::validate_nvidia_smi()?;

        // Release the lock by running nvidia-smi with appropriate flags
        let args = Self::sanitize_args(&["-pm", "0"]);

        let status = Command::new("sudo")
            .arg("nvidia-smi")
            .args(&args)
            .status()
            .context("Failed to execute nvidia-smi for lock release")?;

        if !status.success() {
            anyhow::bail!("Failed to release NVIDIA SMI lock");
        }

        self.lock_acquired = false;
        println!("âœ… Released NVIDIA SMI lock successfully");
        Ok(())
    }
}

/// GPU guard with safety protocols.
pub struct GpuGuard;

impl GpuGuard {
    /// Engages the safety locks for GPU configuration.
    pub fn engage_safety_locks() -> Result<()> {
        println!("ðŸ›¡ï¸  Engaging 4090 Safety Protocols...");

        // 1. Persistence Mode
        Self::run_nvidia_cmd(&["-pm", "1"])?;

        // 2. Power Limit (300W)
        Self::run_nvidia_cmd(&["-pl", "300"])?;

        // 3. Lock Clocks (2100MHz)
        Self::run_nvidia_cmd(&["-lgc", "2100"])?;

        println!("âœ… GPU Locked and Loaded.");
        Ok(())
    }

    /// Runs an nvidia-smi command with the given arguments.
    fn run_nvidia_cmd(args: &[&str]) -> Result<()> {
        let status = Command::new("sudo")
            .arg("nvidia-smi")
            .args(args)
            .status()
            .context("Failed to execute nvidia-smi")?;

        if !status.success() {
            anyhow::bail!("nvidia-smi command failed: {:?}", args);
        }
        Ok(())
    }

    /// Checks if the GPU is idle (under 5% utilization).
    pub fn is_gpu_idle() -> bool {
        // Use nvidia-smi query to check utilization
        let output = Command::new("nvidia-smi")
            .args(&["--query-gpu=utilization.gpu", "--format=csv,noheader,nounits"])
            .output();

        match output {
            Ok(output) => {
                let stdout = String::from_utf8_lossy(&output.stdout);
                if let Ok(util) = stdout.trim().parse::<u32>() {
                    return util < 5; // Idle if under 5% usage
                }
                false
            }
            Err(_) => false, // Return false if command fails
        }
    }

    /// Monitors system resource usage and returns current metrics.
    pub fn monitor_resources() -> Result<HardwareMetrics> {
        let mut metrics = HardwareMetrics::new();

        // Check GPU utilization
        let gpu_output = Command::new("nvidia-smi")
            .args(&["--query-gpu=utilization.gpu", "--format=csv,noheader,nounits"])
            .output()
            .context("Failed to query GPU utilization")?;

        if gpu_output.status.success() {
            let stdout = String::from_utf8_lossy(&gpu_output.stdout);
            if let Ok(util) = stdout.trim().parse::<u32>() {
                metrics.gpu_utilization = util;
            }
        }

        // Check memory usage
        let mem_output = Command::new("nvidia-smi")
            .args(&["--query-gpu=memory.used,.memory.total", "--format=csv,noheader,nounits"])
            .output()
            .context("Failed to query GPU memory")?;

        if mem_output.status.success() {
            let stdout = String::from_utf8_lossy(&mem_output.stdout);
            let parts: Vec<&str> = stdout.trim().split(',').collect();
            if parts.len() >= 2 {
                if let Ok(used) = parts[0].trim().parse::<u32>() {
                    metrics.gpu_memory_used = used;
                }
                if let Ok(total) = parts[1].trim().parse::<u32>() {
                    metrics.gpu_memory_total = total;
                }
            }
        }

        // Check system CPU and memory
        let cpu_output = Command::new("top")
            .args(&["-bn1", "-p", "0"])
            .output()
            .context("Failed to query CPU usage")?;

        if cpu_output.status.success() {
            // Simplified - just note that we're monitoring
            metrics.cpu_usage = 0; // Placeholder
        }

        Ok(metrics)
    }
}

/// Hardware metrics for system resource monitoring.
#[derive(Debug, Clone)]
pub struct HardwareMetrics {
    /// Current GPU utilization percentage
    pub gpu_utilization: u32,
    /// GPU memory used in MB
    pub gpu_memory_used: u32,
    /// Total GPU memory in MB
    pub gpu_memory_total: u32,
    /// CPU usage percentage (placeholder)
    pub cpu_usage: u32,
}

impl HardwareMetrics {
    /// Creates a new hardware metrics instance.
    pub fn new() -> Self {
        Self {
            gpu_utilization: 0,
            gpu_memory_used: 0,
            gpu_memory_total: 0,
            cpu_usage: 0,
        }
    }
}

/// Configuration and setup for GPU resources.
pub struct GpuConfig {
    // Fields for GPU configuration
}

impl GpuConfig {
    /// Creates a new GPU configuration instance.
    pub fn new() -> Self {
        Self {
            // Initialize fields
        }
    }

    /// Sets up NVIDIA GPU settings like persistence mode.
    pub async fn setup_gpu(&self) -> Result<(), Box<dyn std::error::Error>> {
        // Implementation for GPU setup (nvidia-smi settings)
        println!("Setting up NVIDIA GPU configuration...");
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_nv_smi_lock_manager_creation() {
        let manager = NvSmiLockManager::new();
        assert!(true); // Just checking it compiles
    }

    #[tokio::test]
    async fn test_nv_smi_lock_manager_acquire_lock() {
        // Test that we can at least validate the nvidia-smi command exists
        let result = NvSmiLockManager::validate_nvidia_smi();
        // We can't actually acquire a lock in tests, but we verify validation works
        assert!(result.is_ok() || result.is_err()); // Either way is fine for compilation test
    }

    #[tokio::test]
    async fn test_nv_smi_lock_manager_release_lock() {
        let manager = NvSmiLockManager::new();
        // Test that release method can be called without errors (even if no lock was acquired)
        let result = manager.release_lock().await;
        assert!(result.is_ok()); // Should not error even if no lock
    }

    #[tokio::test]
    async fn test_gpu_guard_engage_safety_locks() {
        // This test ensures compilation and basic flow
        // Actual command execution requires sudo permissions in real environments
        let result = GpuGuard::engage_safety_locks();
        // We can't actually test the commands without sudo, but we verify it compiles
        assert!(result.is_ok() || result.is_err()); // Either way is fine for compilation test
    }

    #[tokio::test]
    async fn test_gpu_guard_is_gpu_idle() {
        // Test the is_gpu_idle function - it should return a boolean
        let result = GpuGuard::is_gpu_idle();
        assert!(result == true || result == false); // Should be a boolean value
    }

    #[tokio::test]
    async fn test_gpu_config_creation() {
        let config = GpuConfig::new();
        assert!(true); // Just checking it compiles and can be created
    }

    #[tokio::test]
    async fn test_gpu_config_setup_gpu() {
        let config = GpuConfig::new();
        // This is a basic test - in real scenario we'd mock the actual GPU setup
        let result = config.setup_gpu().await;
        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_hardware_module_structural_integrity() {
        // Test that all structures can be created and have proper signatures
        let _manager = NvSmiLockManager::new();
        let _config = GpuConfig::new();

        // Test that methods exist with correct signatures
        assert!(true); // Just structural test
    }

    #[tokio::test]
    async fn test_hardware_sanitization() {
        // Test input sanitization for command arguments
        let args = &["-pm", "1"];
        let sanitized = NvSmiLockManager::sanitize_args(args);
        assert!(!sanitized.is_empty());

        // Check that dangerous characters are removed
        let test_arg = "test;rm -rf";
        let sanitized_test = NvSmiLockManager::sanitize_args(&[test_arg]);
        assert!(sanitized_test[0].contains("test"));
        assert!(!sanitized_test[0].contains(";"));
    }

    #[tokio::test]
    async fn test_hardware_metrics() {
        // Test hardware metrics collection
        let result = GpuGuard::monitor_resources();
        // This is a basic structural test - actual monitoring may fail in test environment
        assert!(result.is_ok() || result.is_err());
    }
}